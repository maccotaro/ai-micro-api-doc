"""
Base document processor class.
Main orchestrator for document processing operations using modular components.
"""

import json
import shutil
from pathlib import Path
from typing import Dict, List, Any
import logging
from datetime import datetime
import gc
import psutil
import os
import asyncio
import time

from .file_manager import FileManager
from .image_processor import ImageProcessor
from .layout_extractor import LayoutExtractor
from .text_extractor import TextExtractor
from .docling_processor import DoclingProcessor
from .image_cropper import ImageCropper

logger = logging.getLogger(__name__)


class DocumentProcessor:
    """Main document processor class that orchestrates all processing operations"""
    
    def __init__(self, use_layout_extractor: bool = True):
        """Initialize DocumentProcessor.
        
        Args:
            use_layout_extractor: Whether to use layout extraction (default: True)
        """
        self.output_base_dir = Path("/tmp/document_processing")
        self.output_base_dir.mkdir(exist_ok=True)
        
        # Cache directories
        self.easyocr_cache_dir = Path(os.environ.get("EASYOCR_MODULE_PATH", "/tmp/.easyocr_models"))
        self.docling_cache_dir = Path(os.environ.get("DOCLING_CACHE_DIR", "/tmp/.docling_cache"))
        self.easyocr_cache_dir.mkdir(exist_ok=True)
        self.docling_cache_dir.mkdir(exist_ok=True)
        
        # Layout extractor flag
        self.use_layout_extractor = use_layout_extractor
        
        # Initialize modular components
        self.file_manager = FileManager(self.output_base_dir)
        self.image_processor = ImageProcessor()
        if self.use_layout_extractor:
            self.layout_extractor = LayoutExtractor()
        else:
            self.layout_extractor = None
            logger.info("Layout Extractor is disabled")
        self.text_extractor = TextExtractor(self.easyocr_cache_dir)
        self.docling_processor = DoclingProcessor(self.docling_cache_dir)
        self.image_cropper = ImageCropper()
        
        logger.info(f"DocumentProcessor initialized with modular components (layout_extractor={use_layout_extractor})")

    def _log_progress_with_timing(self, description: str, step: int, total: int, start_time: float = None, progress_callback=None):
        """é€²æ—ãƒ­ã‚°ã‚’ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ä»˜ãã§å‡ºåŠ›"""
        current_time = time.time()
        elapsed = round(current_time - start_time, 2) if start_time else 0

        # progress_callbackã‚’å‘¼ã³å‡ºã—
        if progress_callback:
            progress_callback(step=step, total=total, description=description)

        # è©³ç´°ãªé€²æ—ãƒ­ã‚°ã‚’å‡ºåŠ›
        progress_log = {
            "timestamp": datetime.now().isoformat(),
            "component": "DocumentProcessor",
            "operation": "processing_step",
            "step": step,
            "total_steps": total,
            "percentage": round((step / total) * 100, 1) if total > 0 else 0,
            "description": description,
            "elapsed_seconds": elapsed,
            "frontend_format": {
                "status_text": description,
                "step_display": f"ã‚¹ãƒ†ãƒƒãƒ— {step}/{total}",
                "percentage_display": f"{(step / total) * 100:.1f}%" if total > 0 else "0.0%"
            }
        }
        logger.info(f"ğŸ“ˆ DOCUMENT_PROGRESS: {json.dumps(progress_log, ensure_ascii=False)}")

    def _check_memory_usage(self) -> None:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            
            logger.info(f"Current memory usage: {memory_mb:.1f}MB")
            
            # 4GBä»¥ä¸Šä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œï¼ˆ12GBåˆ¶é™ã«èª¿æ•´ï¼‰
            if memory_mb > 4000:
                logger.warning(f"High memory usage detected: {memory_mb:.1f}MB, forcing garbage collection")
                gc.collect()
                # GCå¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å†ç¢ºèª
                new_memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                logger.info(f"Memory usage after GC: {new_memory_mb:.1f}MB (freed: {memory_mb - new_memory_mb:.1f}MB)")
                
                # 10GBä»¥ä¸Šã®å ´åˆã¯è­¦å‘Šï¼ˆ12GBåˆ¶é™ã®83%ï¼‰
                if new_memory_mb > 10000:
                    logger.error(f"Critical memory usage: {new_memory_mb:.1f}MB - approaching 12GB container limit")
                    raise MemoryError(f"Memory usage too high: {new_memory_mb:.1f}MB")
        except Exception as e:
            logger.warning(f"Failed to check memory usage: {e}")
    
    async def process_document_async(self, file_path: str, original_filename: str) -> Dict[str, Any]:
        """Process document asynchronously."""
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                self.process_document,
                file_path,
                original_filename
            )
            return result
        except Exception as e:
            logger.error(f"Async document processing failed: {e}")
            raise
    
    def process_document_with_progress(self, document_path: str, original_filename: str = None, progress_callback=None) -> Dict[str, Any]:
        """ãƒ¡ã‚¤ãƒ³æ–‡æ›¸å‡¦ç†é–¢æ•°ï¼ˆé€²æ—å ±å‘Šä»˜ãï¼‰"""
        start_time = time.time()
        self._log_progress_with_timing("åˆæœŸåŒ–ä¸­...", 0, 10, start_time, progress_callback)

        start_datetime = datetime.now()
        timestamp = start_datetime.strftime("%Y%m%d_%H%M%S")
        
        try:
            self._check_memory_usage()
            
            doc_path = Path(document_path)
            if not doc_path.exists():
                raise FileNotFoundError(f"Document not found: {document_path}")
            
            if original_filename is None:
                original_filename = doc_path.name
            
            logger.info(f"Starting document processing: {original_filename}")

            self._log_progress_with_timing("ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†", 1, 10, start_time, progress_callback)
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            output_dir = self.file_manager.create_output_directory(timestamp, original_filename)
            
            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            self.file_manager.save_original_file(document_path, output_dir, original_filename)

            self._log_progress_with_timing("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†", 2, 10, start_time, progress_callback)
            
            # Doclingå‡¦ç†ã‚’è©¦è¡Œ
            try:
                logger.info("Attempting Docling processing...")
                self._log_progress_with_timing("Doclingå¤‰æ›é–‹å§‹... (æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)", 3, 10, start_time, progress_callback)

                # DoclingåˆæœŸåŒ–ã®é€²æ—ã‚’å ±å‘Š
                self._log_progress_with_timing("Doclingãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...", 3, 10, start_time, progress_callback)

                document = self.docling_processor.convert_document(document_path, progress_callback, str(self.output_base_dir))

                self._log_progress_with_timing("Doclingå¤‰æ›å®Œäº†ã€çµæœã‚’æ¤œè¨¼ä¸­...", 5, 10, start_time, progress_callback)

                if not self.docling_processor.validate_document(document):
                    raise ValueError("Invalid document structure")

                self._log_progress_with_timing("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹é€ æ¤œè¨¼å®Œäº†", 6, 10, start_time, progress_callback)
                
                return self._process_with_docling_progress(document, output_dir, timestamp, original_filename, progress_callback)
                
            except Exception as e:
                logger.warning(f"Docling processing failed: {e}")
                logger.info("Falling back to basic processing...")
                self._log_progress_with_timing("Doclingå‡¦ç†ã«å¤±æ•—ã€åŸºæœ¬å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆä¸­...", 8, 10, start_time, progress_callback)
                return self.file_manager.create_fallback_output(doc_path, output_dir, timestamp, original_filename)
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            self._log_progress_with_timing(f"ã‚¨ãƒ©ãƒ¼: {str(e)}", 10, 10, start_time, progress_callback)
            raise
        
        finally:
            self._check_memory_usage()
            # Cleanup
            self.docling_processor.cleanup()

    def process_document(self, document_path: str, original_filename: str = None) -> Dict[str, Any]:
        """ãƒ¡ã‚¤ãƒ³æ–‡æ›¸å‡¦ç†é–¢æ•°ï¼ˆDocling + othersystemæº–æ‹ ï¼‰"""
        return self.process_document_with_progress(document_path, original_filename, None)

    def _process_with_docling_progress(self, document, output_dir: Path, timestamp: str, original_filename: str, progress_callback=None) -> Dict[str, Any]:
        """Doclingã‚’ä½¿ç”¨ã—ãŸæ–‡æ›¸å‡¦ç†ï¼ˆé€²æ—å ±å‘Šä»˜ãï¼‰"""
        logger.info("Processing document with Docling")
        process_start_time = time.time()

        self._log_progress_with_timing("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è§£æä¸­...", 6, 10, process_start_time, progress_callback)
        
        # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        doc_metadata = self.docling_processor.extract_document_metadata(document)
        num_pages = doc_metadata.get("total_pages", 0)
        
        # PDFæ¬¡å…ƒæƒ…å ±ã‚’æ—©æœŸå–å¾—ï¼ˆãƒšãƒ¼ã‚¸å‡¦ç†ã§åº§æ¨™å¤‰æ›ã«ä½¿ç”¨ï¼‰
        early_page_dimensions = None
        early_image_dimensions = None
        
        if hasattr(document, 'page_dimensions') and document.page_dimensions:
            try:
                # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã‚µã‚¤ã‚ºã‚’å–å¾—
                first_page_dim = document.page_dimensions[0]
                early_page_dimensions = {
                    "width": first_page_dim.width,
                    "height": first_page_dim.height
                }
                logger.info(f"Early PDF page dimensions: {first_page_dim.width}x{first_page_dim.height}")
            except Exception as e:
                logger.warning(f"Failed to get early PDF dimensions: {e}")
        
        # DISABLED: raw_pages.json generation - not used by any endpoint, saves storage space
        # # Doclingç”Ÿãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦raw_pages.jsonã¨ã—ã¦ä¿å­˜
        # try:
        #     raw_pages_data = self.docling_processor.extract_raw_pages_data(document)
        #     raw_pages_file = output_dir / "raw_pages.json"
        #     with open(raw_pages_file, 'w', encoding='utf-8') as f:
        #         import json
        #         json.dump(raw_pages_data, f, ensure_ascii=False, indent=2)
        #     logger.info(f"Raw pages data saved: {raw_pages_file}")
        # except Exception as e:
        #     logger.warning(f"Failed to save raw pages data: {e}")
        
        if num_pages == 0:
            logger.warning("No pages found in document")
            self._log_progress_with_timing("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ä¸­...", 10, 10, process_start_time, progress_callback)
            return self.file_manager.create_fallback_output(
                Path(document._original_pdf_path), output_dir, timestamp, original_filename
            )

        self._log_progress_with_timing(f"{num_pages}ãƒšãƒ¼ã‚¸ã®å‡¦ç†ã‚’é–‹å§‹...", 7, 10, process_start_time, progress_callback)
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Doclingã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰çµ±åˆæ§‹é€ ã‚’åˆ†æ
        unified_structure = None
        try:
            from .document_structure_analyzer import DocumentStructureAnalyzer
            structure_analyzer = DocumentStructureAnalyzer()
            unified_structure = structure_analyzer.create_unified_structure_from_docling(
                document, num_pages
            )
            logger.info(f"Unified structure created with {len(unified_structure.get('sections', []))} sections")
        except Exception as e:
            logger.warning(f"Failed to create unified structure: {e}")
            unified_structure = None
        
        pages_data = []
        total_elements = 0
        
        # æ–‡æ›¸å…¨ä½“ã§å…±æœ‰ã™ã‚‹HierarchyConverterã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        from .hierarchical_extractor import HierarchicalExtractor
        from .hierarchy_converter import HierarchyConverter
        document_hierarchy_converter = HierarchyConverter()  # æ–‡æ›¸å…¨ä½“ã§é€šã—ç•ªå·ã‚’ç®¡ç†
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å„ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ï¼ˆçµ±åˆæ§‹é€ ã‚’å‚ç…§ã—ãªãŒã‚‰ï¼‰
        for page_num in range(num_pages):
            try:
                page_progress = 7 + (page_num / num_pages) * 2  # 7ã‹ã‚‰9ã®é–“ã§é€²æ—
                if progress_callback:
                    progress_callback(
                        step=int(page_progress), 
                        total=10, 
                        description=f"ãƒšãƒ¼ã‚¸ {page_num + 1}/{num_pages} ã‚’å‡¦ç†ä¸­..."
                    )
                
                logger.info(f"Processing page {page_num + 1}/{num_pages}")
                
                # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæŠ½å‡º (doclingå‡¦ç†)
                if self.use_layout_extractor and self.layout_extractor:
                    page_layout = self.layout_extractor.extract_page_layout(document, page_num, output_dir)
                else:
                    # Layout ExtractorãŒç„¡åŠ¹ã®å ´åˆã¯ç©ºã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¿”ã™
                    page_layout = {
                        "page_number": page_num + 1,
                        "elements": [],
                        "raw_docling_elements": []  # Doclingã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                    }
                    logger.info(f"Layout extraction skipped for page {page_num + 1}")
                
                # çµ±åˆæ§‹é€ ã‹ã‚‰è©²å½“ãƒšãƒ¼ã‚¸ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
                if unified_structure and "sections" in unified_structure:
                    page_sections = [
                        section["section_id"] 
                        for section in unified_structure["sections"]
                        if section["start_page"] <= page_num + 1 <= section["end_page"]
                    ]
                    page_layout["sections"] = page_sections
                    page_layout["document_structure_ref"] = "metadata_ext.json#/unified_document_structure"
                
                # ã‚¹ãƒ†ãƒƒãƒ—2: éšå±¤æ§‹é€ å¤‰æ› (hierarchyå‡¦ç†)
                # ãƒšãƒ¼ã‚¸ãƒ¬ãƒ™ãƒ«ã§éšå±¤æ§‹é€ ã‚’ç”Ÿæˆã—ã€hierarchical_elementsã‚’ä½œæˆ
                try:
                    # ã¾ãšæ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’éƒ¨åˆ†çš„ã«ä½œæˆï¼ˆã“ã®ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
                    hierarchical_extractor = HierarchicalExtractor()
                    temp_metadata = {
                        "document_name": original_filename,
                        "processing_timestamp": timestamp,
                        "total_pages": num_pages,
                        "pages": [{"page_number": page_num + 1}]  # ä¸€æ™‚çš„ãªå˜ä¸€ãƒšãƒ¼ã‚¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    }
                    
                    # dimensionsæƒ…å ±ã‚’è¿½åŠ ï¼ˆåº§æ¨™å¤‰æ›ã§ä½¿ç”¨ï¼‰
                    if early_page_dimensions:
                        temp_metadata["dimensions"] = {
                            "pdf_page": early_page_dimensions,
                            "image_page": early_image_dimensions
                        }
                    
                    # DISABLED: metadata_ext.json generation - not used by frontend, all data in metadata_hierarchy.json
                    # # ãƒšãƒ¼ã‚¸æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                    # page_metadata_ext = hierarchical_extractor.create_extended_metadata(
                    #     document, output_dir, temp_metadata, original_filename, single_page=page_num
                    # )
                    page_metadata_ext = temp_metadata  # temp_metadataã«ã¯dimensionsæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹
                    
                    # æ–‡æ›¸å…¨ä½“ã§å…±æœ‰ã™ã‚‹HierarchyConverterã§ãƒšãƒ¼ã‚¸ãƒ¬ãƒ™ãƒ«å¤‰æ›ã‚’å®Ÿè¡Œ
                    page_layout = document_hierarchy_converter.convert_page_to_hierarchy(
                        page_layout, page_num + 1, page_metadata_ext, document
                    )
                    
                    logger.info(f"âœ… Page {page_num + 1}: Created hierarchical_elements ({len(page_layout.get('hierarchical_elements', []))} elements)")
                    
                except Exception as e:
                    logger.warning(f"Failed to create hierarchical structure for page {page_num + 1}: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚‚ã¨ã®page_layoutã‚’ãã®ã¾ã¾ä½¿ç”¨
                
                # DISABLED: Individual layout file creation - data included in metadata_hierarchy.json
                # layout_file = self.file_manager.create_layout_file(output_dir, page_num + 1, page_layout)
                layout_file = f"layout/page_{page_num + 1}_layout.json"  # Path reference only
                
                # ãƒšãƒ¼ã‚¸ç”»åƒä½œæˆ
                images_dir = output_dir / "images"
                page_images = self.image_processor.create_page_image(page_num, images_dir, document)
                
                # ã‚¹ãƒ†ãƒƒãƒ—3: ç”»åƒåˆ‡ã‚Šå‡ºã— (cropå‡¦ç†)
                # hierarchical_elementsãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°elementsã‚’ä½¿ç”¨
                cropped_elements = []
                has_hierarchical = page_layout.get("hierarchical_elements") is not None
                elements_source = page_layout.get("hierarchical_elements") if has_hierarchical else page_layout.get("elements", [])
                logger.info(f"ğŸ“‹ Page {page_num + 1}: Using {'hierarchical_elements' if has_hierarchical else 'elements'} ({len(elements_source)} elements)")
                
                if elements_source:
                    page_image_path = images_dir / f"page_{page_num + 1}_full.png"
                    if page_image_path.exists():
                        # ç”»åƒè¦ç´ ï¼ˆfigure, picture, table, captionï¼‰ã‚’æŠ½å‡ºã—ã¦IDã‚’ç¢ºèª
                        croppable_elements = [elem for elem in elements_source if elem.get("type") in ["figure", "picture", "image", "table", "caption"]]
                        
                        # IDãŒãªã„è¦ç´ ã«IDã‚’ç”Ÿæˆ
                        for elem in croppable_elements:
                            if not elem.get("id"):
                                elem["id"] = f"auto-{page_num + 1}-{elem.get('type', 'unknown')}-{id(elem)}"
                                logger.info(f"ğŸ†” Generated ID for element: {elem['id']}")
                        
                        logger.info(f"ğŸ–¼ï¸ Found {len(croppable_elements)} croppable elements on page {page_num + 1}")
                        
                        # æ–°ã—ã„crop_single_elementæ–¹å¼ã§å„è¦ç´ ã‚’å‡¦ç†
                        if croppable_elements:
                            try:
                                cropped_count = 0
                                for elem in croppable_elements:
                                    if self.image_cropper.crop_single_element(
                                        str(page_image_path), elem, str(images_dir), scale_factor=2.0
                                    ):
                                        cropped_elements.append(elem)
                                        cropped_count += 1
                                        logger.info(f"âœ… Successfully cropped element {elem.get('id')} ({elem.get('type')})")
                                    else:
                                        logger.warning(f"âŒ Failed to crop element {elem.get('id')} ({elem.get('type')})")
                                
                                logger.info(f"ğŸ“Š Successfully cropped {cropped_count}/{len(croppable_elements)} elements on page {page_num + 1}")
                                
                            except Exception as e:
                                logger.warning(f"Failed to crop elements on page {page_num + 1}: {e}")
                
                # æ³¨é‡ˆä»˜ãç”»åƒä½œæˆï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ãŒã‚ã‚‹å ´åˆï¼‰
                if page_layout.get("hierarchical_elements"):
                    page_image_path = images_dir / f"page_{page_num + 1}_full.png"
                    annotated_image_path = images_dir / f"page_{page_num + 1}_full_annotated.png"
                    
                    if page_image_path.exists():
                        self.image_processor.create_annotated_image(
                            page_image_path, page_layout, annotated_image_path, scale_factor=2.0
                        )
                
                # DISABLED: Individual text file creation - data included in metadata_hierarchy.json
                page_text = self._extract_page_text(document, page_num, page_layout)
                # text_file = self.file_manager.create_text_file(output_dir, page_num + 1, page_text)
                text_file = f"text/page_{page_num + 1}.txt"  # Path reference only
                
                # ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                page_data = {
                    "page_number": page_num + 1,
                    "layout_file": layout_file,  # Direct string path
                    "image_files": [img["file"] for img in page_images] if page_images else [],
                    "text_file": text_file,  # Direct string path
                    "text_content": page_text[:500] + "..." if len(page_text) > 500 else page_text,
                    "hierarchical_elements": page_layout.get("hierarchical_elements", []),
                    "has_hierarchy": True,
                    "cropped_elements_count": len(cropped_elements),
                    "cropped_figure_count": len([elem for elem in cropped_elements if elem.get("type") in ["figure", "picture", "image"]]),
                    "cropped_table_count": len([elem for elem in cropped_elements if elem.get("type") == "table"])
                }
                
                pages_data.append(page_data)
                total_elements += len(page_layout.get("hierarchical_elements", []))
                
            except Exception as e:
                logger.error(f"Failed to process page {page_num + 1}: {e}")
                # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                pages_data.append({
                    "page_number": page_num + 1,
                    "layout_file": None,
                    "image_files": [],
                    "text_file": None,
                    "text_content": f"Error processing page {page_num + 1}",
                    "hierarchical_elements": [],
                    "has_hierarchy": False
                })
        
        self._log_progress_with_timing("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­...", 9, 10, process_start_time, progress_callback)
        
        # ç”»åƒã‚µã‚¤ã‚ºã¨PDFã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—
        page_dimensions = None
        image_dimensions = None
        
        # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—
        if pages_data:
            first_page_image = images_dir / f"page_1_full.png"
            if first_page_image.exists():
                try:
                    from PIL import Image
                    with Image.open(first_page_image) as img:
                        image_dimensions = {
                            "width": img.width,
                            "height": img.height
                        }
                        logger.info(f"First page image dimensions: {img.width}x{img.height}")
                except Exception as e:
                    logger.warning(f"Failed to get image dimensions: {e}")
        
        # PDFãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’å–å¾—ï¼ˆDoclingã‹ã‚‰ï¼‰
        if hasattr(document, 'page_dimensions') and document.page_dimensions:
            try:
                # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã‚µã‚¤ã‚ºã‚’å–å¾—
                first_page_dim = document.page_dimensions[0]
                page_dimensions = {
                    "width": first_page_dim.width,
                    "height": first_page_dim.height
                }
                logger.info(f"PDF page dimensions: {first_page_dim.width}x{first_page_dim.height}")
            except Exception as e:
                logger.warning(f"Failed to get PDF dimensions: {e}")
        
        # ç·åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = {
            "document_name": original_filename,
            "processing_timestamp": timestamp,
            "total_pages": num_pages,
            "total_elements": total_elements,
            "pages": pages_data,
            "processing_mode": "docling",
            "docling_metadata": doc_metadata,
            "text_extraction": {
                "method": "docling",
                "ocr_enabled": False
            },
            "dimensions": {
                "pdf_page": page_dimensions,
                "image_page": image_dimensions
            }
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆå‰Šé™¤äºˆå®šï¼šmetadata_hierarchy.jsonã«çµ±åˆï¼‰
        # metadata_file = self.file_manager.create_metadata_file(output_dir, metadata)
        
        # çµ±åˆæ–‡æ›¸æ§‹é€ åˆ†æã¨æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        try:
            from .hierarchical_extractor import HierarchicalExtractor
            
            # æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆæ—¢å­˜ã®éšå±¤æ§‹é€ ä»˜ãï¼‰
            hierarchical_extractor = HierarchicalExtractor()
            extended_metadata = hierarchical_extractor.create_extended_metadata(
                document, output_dir, metadata, original_filename
            )
            
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨éšå±¤æ§‹é€ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            try:
                from .hierarchy_converter import HierarchyConverter
                
                hierarchy_converter = HierarchyConverter()
                
                # å‡¦ç†æ¸ˆã¿ã®pages_dataã‹ã‚‰hierarchical_elementsã‚’å–å¾—ã—ã¦hierarchy_metadataã‚’æ§‹ç¯‰
                hierarchy_metadata = metadata.copy()
                
                # pages_dataã‹ã‚‰å‡¦ç†æ¸ˆã¿ã®hierarchical_elementsã‚’ä½¿ç”¨
                for page_idx, page_data in enumerate(pages_data):
                    if page_idx < len(hierarchy_metadata.get("pages", [])):
                        # å‡¦ç†æ¸ˆã¿ã®hierarchical_elementsã‚’ä½¿ç”¨
                        processed_hierarchical_elements = page_data.get("hierarchical_elements", [])
                        hierarchy_metadata["pages"][page_idx]["hierarchical_elements"] = processed_hierarchical_elements
                        hierarchy_metadata["pages"][page_idx]["has_hierarchy"] = True
                        
                        # cropped_image_pathãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        cropped_count = len([elem for elem in processed_hierarchical_elements 
                                           if elem.get("cropped_image_path") is not None])
                        
                        logger.info(f"ğŸ“‹ Updated page {page_idx + 1}: {len(processed_hierarchical_elements)} elements, "
                                  f"{cropped_count} with cropped images")
                
                logger.info("âœ… Successfully transferred processed hierarchical_elements with cropped_image_path data")
                
                logger.info("âœ… Using processed hierarchical_elements with cropped_image_path data")
                
                # document_structure_summary ã‚’éšå±¤ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                if unified_structure:
                    hierarchy_metadata["document_structure_summary"] = {
                        "total_sections": len(unified_structure.get("sections", [])),
                        "document_type": unified_structure.get("document_overview", {}).get("document_type", "unknown"),
                        "structure_confidence": unified_structure.get("document_overview", {}).get("structure_confidence", 0.0),
                        "has_unified_structure": True
                    }
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ§‹é€ æƒ…å ±
                    hierarchy_metadata["document_structure_summary"] = {
                        "total_sections": 1,
                        "document_type": "general_document", 
                        "structure_confidence": 0.0,
                        "has_unified_structure": True
                    }
                
                # metadata_hierarchy.json ã‚’ä¿å­˜
                hierarchy_file = hierarchy_converter.save_hierarchy_metadata(
                    hierarchy_metadata, output_dir
                )
                logger.info(f"Created frontend hierarchy metadata: {hierarchy_file}")
                
                # DBä¿å­˜ç”¨ã«éšå±¤ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                self.processed_hierarchy_metadata = hierarchy_metadata
                
            except Exception as hierarchy_e:
                logger.warning(f"Failed to create hierarchy metadata: {hierarchy_e}")
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ã¯ç¶šè¡Œ
            
            # DISABLED: Extended metadata and document structure generation - not used by frontend
            # # çµ±åˆæ–‡æ›¸æ§‹é€ ã‚’æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ï¼ˆã™ã§ã«ä½œæˆæ¸ˆã¿ï¼‰
            # if unified_structure:
            #     extended_metadata["unified_document_structure"] = unified_structure
            # else:
            #     # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä½œæˆ
            #     from .document_structure_analyzer import DocumentStructureAnalyzer
            #     all_page_layouts = []
            #     for page_data in pages_data:
            #         layout_file_path = output_dir / page_data.get("layout_file", "")
            #         if layout_file_path.exists():
            #             try:
            #                 with open(layout_file_path, 'r', encoding='utf-8') as f:
            #                     import json
            #                     layout_data = json.load(f)
            #                     all_page_layouts.append(layout_data)
            #             except Exception as e:
            #                 logger.warning(f"Failed to load layout file {layout_file_path}: {e}")
            #     
            #     structure_analyzer = DocumentStructureAnalyzer()
            #     unified_structure = structure_analyzer.create_unified_document_structure(
            #         document, all_page_layouts, metadata
            #     )
            #     extended_metadata["unified_document_structure"] = unified_structure
            # 
            # # metadata_ext.jsonã¨ã—ã¦ä¿å­˜
            # metadata_ext_file = output_dir / "metadata_ext.json"
            # with open(metadata_ext_file, 'w', encoding='utf-8') as f:
            #     import json
            #     json.dump(extended_metadata, f, ensure_ascii=False, indent=2)
            # 
            # logger.info(f"Extended metadata with unified structure created: {metadata_ext_file}")
            
            # DISABLED: Structure visualization files generation - not used by frontend, saves storage space
            # # æ§‹é€ è¦–è¦šåŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
            # try:
            #     from .structure_visualizer import DocumentStructureVisualizer
            #     visualizer = DocumentStructureVisualizer(metadata_ext_file)
            #     
            #     # éšå±¤ãƒ„ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
            #     visualizer.generate_hierarchy_tree(output_dir)
            #     logger.info("Generated hierarchy tree text file")
            #     
            #     # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–HTMLãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆ
            #     visualizer.generate_html_viewer(output_dir)
            #     logger.info("Generated interactive HTML viewer")
            #     
            #     # Mermaidãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ç”Ÿæˆ
            #     visualizer.generate_mermaid_diagram(output_dir)
            #     logger.info("Generated Mermaid diagram")
            #     
            # except Exception as e:
            #     logger.warning(f"Could not generate structure visualization: {e}")
            
            # é‡è¤‡å‰Šé™¤ï¼šdocument_structure_summaryã¯æ—¢ã«hierarchy_metadataã«è¿½åŠ æ¸ˆã¿
            # metadata.jsonã®é‡è¤‡ä½œæˆã‚’å‰Šé™¤ï¼ˆmetadata_hierarchy.jsonã«çµ±åˆï¼‰
            
        except Exception as e:
            logger.warning(f"Failed to create unified document structure: {e}")
            # çµ±åˆæ§‹é€ ã®ä½œæˆã«å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã¯ç¶šè¡Œ
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ—¢å­˜ã®æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½œæˆ
                from .hierarchical_extractor import HierarchicalExtractor
                hierarchical_extractor = HierarchicalExtractor()
                
                extended_metadata = hierarchical_extractor.create_extended_metadata(
                    document, output_dir, metadata, original_filename
                )
                
                metadata_ext_file = output_dir / "metadata_ext.json"
                with open(metadata_ext_file, 'w', encoding='utf-8') as f:
                    import json
                    json.dump(extended_metadata, f, ensure_ascii=False, indent=2)
                
                logger.info(f"Fallback extended metadata created: {metadata_ext_file}")
                
            except Exception as fallback_e:
                logger.warning(f"Failed to create fallback extended metadata: {fallback_e}")
        
        # å‡¦ç†æ™‚é–“è¨ˆç®—
        processing_time = (datetime.now() - datetime.fromisoformat(timestamp.replace('_', '')[:8] + 'T' + timestamp.replace('_', '')[8:10] + ':' + timestamp.replace('_', '')[10:12] + ':' + timestamp.replace('_', '')[12:14])).total_seconds()
        
        self._log_progress_with_timing("å‡¦ç†å®Œäº†!", 10, 10, process_start_time, progress_callback)
        
        logger.info(f"Docling processing completed: {total_elements} elements extracted in {processing_time:.1f}s")
        
        return {
            "status": "success",
            "output_directory": str(output_dir.relative_to(self.output_base_dir.parent)),
            "files_created": {
                "original": str((output_dir / "original" / original_filename).relative_to(self.output_base_dir.parent)),
                "metadata_hierarchy": str((output_dir / "metadata_hierarchy.json").relative_to(self.output_base_dir.parent)),
                "layout_dir": str((output_dir / "layout").relative_to(self.output_base_dir.parent)),
                "images_dir": str((output_dir / "images").relative_to(self.output_base_dir.parent)),
                "text_dir": str((output_dir / "text").relative_to(self.output_base_dir.parent))
            },
            "total_pages": num_pages,
            "total_elements": total_elements,
            "processing_mode": "docling",
            "processing_time": f"{processing_time:.1f}s",
            "original_filename": original_filename,
            "metadata": getattr(self, 'processed_hierarchy_metadata', None)  # DBä¿å­˜ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        }
    
    def _process_with_docling(self, document, output_dir: Path, timestamp: str, original_filename: str) -> Dict[str, Any]:
        """Doclingã‚’ä½¿ç”¨ã—ãŸæ–‡æ›¸å‡¦ç†"""
        logger.info("Processing document with Docling")
        
        # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        doc_metadata = self.docling_processor.extract_document_metadata(document)
        num_pages = doc_metadata.get("total_pages", 0)
        
        if num_pages == 0:
            logger.warning("No pages found in document")
            return self.file_manager.create_fallback_output(
                Path(document._original_pdf_path), output_dir, timestamp, original_filename
            )
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Doclingã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰çµ±åˆæ§‹é€ ã‚’åˆ†æ
        unified_structure = None
        try:
            from .document_structure_analyzer import DocumentStructureAnalyzer
            structure_analyzer = DocumentStructureAnalyzer()
            unified_structure = structure_analyzer.create_unified_structure_from_docling(
                document, num_pages
            )
            logger.info(f"Unified structure created with {len(unified_structure.get('sections', []))} sections")
        except Exception as e:
            logger.warning(f"Failed to create unified structure: {e}")
            unified_structure = None
        
        pages_data = []
        total_elements = 0
        
        # å„ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†
        for page_num in range(num_pages):
            try:
                logger.info(f"Processing page {page_num + 1}/{num_pages}")
                
                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæŠ½å‡º
                if self.use_layout_extractor and self.layout_extractor:
                    page_layout = self.layout_extractor.extract_page_layout(document, page_num, output_dir)
                else:
                    # Layout ExtractorãŒç„¡åŠ¹ã®å ´åˆã¯ç©ºã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¿”ã™
                    page_layout = {
                        "page_number": page_num + 1,
                        "elements": [],
                        "raw_docling_elements": []  # Doclingã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                    }
                    logger.info(f"Layout extraction skipped for page {page_num + 1}")
                
                # çµ±åˆæ§‹é€ ã‹ã‚‰è©²å½“ãƒšãƒ¼ã‚¸ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
                if unified_structure and "sections" in unified_structure:
                    page_sections = [
                        section["section_id"] 
                        for section in unified_structure["sections"]
                        if section["start_page"] <= page_num + 1 <= section["end_page"]
                    ]
                    page_layout["sections"] = page_sections
                    page_layout["document_structure_ref"] = "metadata_ext.json#/unified_document_structure"
                
                # DISABLED: Individual layout file creation - data included in metadata_hierarchy.json
                # layout_file = self.file_manager.create_layout_file(output_dir, page_num + 1, page_layout)
                layout_file = f"layout/page_{page_num + 1}_layout.json"  # Path reference only
                
                # ãƒšãƒ¼ã‚¸ç”»åƒä½œæˆ
                images_dir = output_dir / "images"
                page_images = self.image_processor.create_page_image(page_num, images_dir, document)
                
                # ã‚¹ãƒ†ãƒƒãƒ—3: ç”»åƒåˆ‡ã‚Šå‡ºã— (cropå‡¦ç†)
                # hierarchical_elementsãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°elementsã‚’ä½¿ç”¨
                cropped_elements = []
                has_hierarchical = page_layout.get("hierarchical_elements") is not None
                elements_source = page_layout.get("hierarchical_elements") if has_hierarchical else page_layout.get("elements", [])
                logger.info(f"ğŸ“‹ Page {page_num + 1}: Using {'hierarchical_elements' if has_hierarchical else 'elements'} ({len(elements_source)} elements)")
                
                if elements_source:
                    page_image_path = images_dir / f"page_{page_num + 1}_full.png"
                    if page_image_path.exists():
                        # ç”»åƒè¦ç´ ï¼ˆfigure, picture, table, captionï¼‰ã‚’æŠ½å‡ºã—ã¦IDã‚’ç¢ºèª
                        croppable_elements = [elem for elem in elements_source if elem.get("type") in ["figure", "picture", "image", "table", "caption"]]
                        
                        # IDãŒãªã„è¦ç´ ã«IDã‚’ç”Ÿæˆ
                        for elem in croppable_elements:
                            if not elem.get("id"):
                                elem["id"] = f"auto-{page_num + 1}-{elem.get('type', 'unknown')}-{id(elem)}"
                                logger.info(f"ğŸ†” Generated ID for element: {elem['id']}")
                        
                        logger.info(f"ğŸ–¼ï¸ Found {len(croppable_elements)} croppable elements on page {page_num + 1}")
                        
                        # æ–°ã—ã„crop_single_elementæ–¹å¼ã§å„è¦ç´ ã‚’å‡¦ç†
                        if croppable_elements:
                            try:
                                cropped_count = 0
                                for elem in croppable_elements:
                                    if self.image_cropper.crop_single_element(
                                        str(page_image_path), elem, str(images_dir), scale_factor=2.0
                                    ):
                                        cropped_elements.append(elem)
                                        cropped_count += 1
                                        logger.info(f"âœ… Successfully cropped element {elem.get('id')} ({elem.get('type')})")
                                    else:
                                        logger.warning(f"âŒ Failed to crop element {elem.get('id')} ({elem.get('type')})")
                                
                                logger.info(f"ğŸ“Š Successfully cropped {cropped_count}/{len(croppable_elements)} elements on page {page_num + 1}")
                                
                            except Exception as e:
                                logger.warning(f"Failed to crop elements on page {page_num + 1}: {e}")
                
                # æ³¨é‡ˆä»˜ãç”»åƒä½œæˆï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ãŒã‚ã‚‹å ´åˆï¼‰
                if page_layout.get("hierarchical_elements"):
                    page_image_path = images_dir / f"page_{page_num + 1}_full.png"
                    annotated_image_path = images_dir / f"page_{page_num + 1}_full_annotated.png"
                    
                    if page_image_path.exists():
                        self.image_processor.create_annotated_image(
                            page_image_path, page_layout, annotated_image_path, scale_factor=2.0
                        )
                
                # DISABLED: Individual text file creation - data included in metadata_hierarchy.json
                page_text = self._extract_page_text(document, page_num, page_layout)
                # text_file = self.file_manager.create_text_file(output_dir, page_num + 1, page_text)
                text_file = f"text/page_{page_num + 1}.txt"  # Path reference only
                
                # ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                page_data = {
                    "page_number": page_num + 1,
                    "layout_file": layout_file,  # Direct string path
                    "image_files": [img["file"] for img in page_images] if page_images else [],
                    "text_file": text_file,  # Direct string path
                    "text_content": page_text[:500] + "..." if len(page_text) > 500 else page_text,
                    "hierarchical_elements": page_layout.get("hierarchical_elements", []),
                    "has_hierarchy": True,
                    "cropped_elements_count": len(cropped_elements),
                    "cropped_figure_count": len([elem for elem in cropped_elements if elem.get("type") in ["figure", "picture", "image"]]),
                    "cropped_table_count": len([elem for elem in cropped_elements if elem.get("type") == "table"])
                }
                
                pages_data.append(page_data)
                total_elements += len(page_layout.get("hierarchical_elements", []))
                
            except Exception as e:
                logger.error(f"Failed to process page {page_num + 1}: {e}")
                # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                pages_data.append({
                    "page_number": page_num + 1,
                    "layout_file": None,
                    "image_files": [],
                    "text_file": None,
                    "text_content": f"Error processing page {page_num + 1}",
                    "hierarchical_elements": [],
                    "has_hierarchy": False
                })
        
        # ç”»åƒã‚µã‚¤ã‚ºã¨PDFã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—ï¼ˆ2ã¤ç›®ã®å ´æ‰€ï¼‰
        page_dimensions = None
        image_dimensions = None
        
        # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—
        if pages_data:
            first_page_image = images_dir / f"page_1_full.png"
            if first_page_image.exists():
                try:
                    from PIL import Image
                    with Image.open(first_page_image) as img:
                        image_dimensions = {
                            "width": img.width,
                            "height": img.height
                        }
                        logger.info(f"First page image dimensions (fallback): {img.width}x{img.height}")
                except Exception as e:
                    logger.warning(f"Failed to get image dimensions (fallback): {e}")
        
        # PDFãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’å–å¾—ï¼ˆDoclingã‹ã‚‰ï¼‰
        if hasattr(document, 'page_dimensions') and document.page_dimensions:
            try:
                # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã‚µã‚¤ã‚ºã‚’å–å¾—
                first_page_dim = document.page_dimensions[0]
                page_dimensions = {
                    "width": first_page_dim.width,
                    "height": first_page_dim.height
                }
                logger.info(f"PDF page dimensions (fallback): {first_page_dim.width}x{first_page_dim.height}")
            except Exception as e:
                logger.warning(f"Failed to get PDF dimensions (fallback): {e}")
        
        # ç·åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = {
            "document_name": original_filename,
            "processing_timestamp": timestamp,
            "total_pages": num_pages,
            "total_elements": total_elements,
            "pages": pages_data,
            "processing_mode": "docling",
            "docling_metadata": doc_metadata,
            "text_extraction": {
                "method": "docling",
                "ocr_enabled": False
            },
            "dimensions": {
                "pdf_page": page_dimensions,
                "image_page": image_dimensions
            }
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆå‰Šé™¤äºˆå®šï¼šmetadata_hierarchy.jsonã«çµ±åˆï¼‰
        # metadata_file = self.file_manager.create_metadata_file(output_dir, metadata)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨éšå±¤æ§‹é€ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆfallbackå‡¦ç†ï¼‰
        try:
            # æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿éšå±¤æ§‹é€ ã‚’ä½œæˆ
            metadata_ext_file = output_dir / "metadata_ext.json"
            if metadata_ext_file.exists():
                with open(metadata_ext_file, 'r', encoding='utf-8') as f:
                    extended_metadata = json.load(f)
                
                from .hierarchy_converter import HierarchyConverter
                
                hierarchy_converter = HierarchyConverter()
                hierarchy_metadata = hierarchy_converter.convert_to_frontend_hierarchy(
                    extended_metadata, metadata, document
                )
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®æ§‹é€ ã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
                hierarchy_metadata["document_structure_summary"] = {
                    "total_sections": 1,
                    "document_type": "general_document",
                    "structure_confidence": 0.0,
                    "has_unified_structure": False
                }
                
                # metadata_hierarchy.json ã‚’ä¿å­˜
                hierarchy_file = hierarchy_converter.save_hierarchy_metadata(
                    hierarchy_metadata, output_dir
                )
                logger.info(f"Created frontend hierarchy metadata (fallback): {hierarchy_file}")
                
                # DBä¿å­˜ç”¨ã«éšå±¤ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                self.processed_hierarchy_metadata = hierarchy_metadata
                
        except Exception as hierarchy_e:
            logger.warning(f"Failed to create hierarchy metadata in fallback: {hierarchy_e}")
        
        # å‡¦ç†æ™‚é–“è¨ˆç®—
        processing_time = (datetime.now() - datetime.fromisoformat(timestamp.replace('_', '')[:8] + 'T' + timestamp.replace('_', '')[8:10] + ':' + timestamp.replace('_', '')[10:12] + ':' + timestamp.replace('_', '')[12:14])).total_seconds()
        
        logger.info(f"Docling processing completed: {total_elements} elements extracted in {processing_time:.1f}s")
        
        return {
            "status": "success",
            "output_directory": str(output_dir.relative_to(self.output_base_dir.parent)),
            "files_created": {
                "original": str((output_dir / "original" / original_filename).relative_to(self.output_base_dir.parent)),
                "metadata_hierarchy": str((output_dir / "metadata_hierarchy.json").relative_to(self.output_base_dir.parent)),
                "layout_dir": str((output_dir / "layout").relative_to(self.output_base_dir.parent)),
                "images_dir": str((output_dir / "images").relative_to(self.output_base_dir.parent)),
                "text_dir": str((output_dir / "text").relative_to(self.output_base_dir.parent))
            },
            "total_pages": num_pages,
            "total_elements": total_elements,
            "processing_mode": "docling",
            "processing_time": f"{processing_time:.1f}s",
            "original_filename": original_filename,
            "metadata": getattr(self, 'processed_hierarchy_metadata', None)  # DBä¿å­˜ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        }
    
    def _extract_page_text(self, document, page_num: int, page_layout: Dict) -> str:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            text_parts = []
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆéšå±¤æ§‹é€ å¯¾å¿œï¼‰
            def extract_text_from_hierarchy(elements):
                for element in elements:
                    element_text = element.get("text", "")
                    if element_text and element_text.strip():
                        text_parts.append(element_text.strip())
                    # å­è¦ç´ ã‚‚å†å¸°çš„ã«å‡¦ç†
                    if element.get("children"):
                        extract_text_from_hierarchy(element["children"])
            
            extract_text_from_hierarchy(page_layout.get("hierarchical_elements", []))
            
            # Docling document ã‹ã‚‰ã®ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not text_parts and hasattr(document, 'pages') and page_num < len(document.pages):
                try:
                    page = document.pages[page_num]
                    if hasattr(page, 'items'):
                        for item in page.items:
                            item_text = self.text_extractor.extract_text_content(item)
                            if item_text:
                                text_parts.append(item_text)
                except Exception as e:
                    logger.warning(f"Failed to extract text from page items: {e}")
            
            return "\\n".join(text_parts) if text_parts else f"Page {page_num + 1} - No text extracted"
            
        except Exception as e:
            logger.warning(f"Failed to extract page text: {e}")
            return f"Page {page_num + 1} - Text extraction failed"


# Global instance
_document_processor = None

def get_document_processor(use_layout_extractor: bool = True) -> DocumentProcessor:
    """Get global document processor instance.
    
    Args:
        use_layout_extractor: Whether to use layout extraction (default: True)
    """
    global _document_processor
    if _document_processor is None or _document_processor.use_layout_extractor != use_layout_extractor:
        _document_processor = DocumentProcessor(use_layout_extractor=use_layout_extractor)
    return _document_processor
"""
Docling-specific processing utilities.
Handles Docling document conversion, configuration, and specialized processing.
"""

import logging
from pathlib import Path
from typing import Dict, Any
import os
import gc
from datetime import datetime
import json
import time

from docling.document_converter import DocumentConverter

logger = logging.getLogger(__name__)

# GPU detection
try:
    import torch
    GPU_AVAILABLE = torch.cuda.is_available()
except ImportError:
    GPU_AVAILABLE = False


class DoclingProcessor:
    """Docling document processing and conversion operations"""
    
    def __init__(self, docling_cache_dir: Path = None):
        self.docling_cache_dir = docling_cache_dir or Path("/tmp/.docling_cache")
        self._document_converter = None
        self._setup_docling_environment()
    
    def _setup_docling_environment(self):
        """Setup Docling environment and cache directories"""
        try:
            # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã«ä½œæˆ
            models_dir = Path("/usr/local/lib/python3.11/site-packages/deepsearch_glm/resources/models/crf/part-of-speech")
            models_dir.mkdir(parents=True, exist_ok=True)
            
            # æ¨©é™è¨­å®š
            os.chmod(models_dir, 0o755)
            
            # HuggingFaceè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã®å ´åˆã®ã¿è¨­å®šï¼‰
            if "HF_HOME" not in os.environ:
                os.environ["HF_HOME"] = str(self.docling_cache_dir / "huggingface")
            if "TRANSFORMERS_CACHE" not in os.environ:
                os.environ["TRANSFORMERS_CACHE"] = str(self.docling_cache_dir / "transformers")
            if "TORCH_HOME" not in os.environ:
                os.environ["TORCH_HOME"] = str(self.docling_cache_dir / "torch")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            self.docling_cache_dir.mkdir(parents=True, exist_ok=True)
            (self.docling_cache_dir / "huggingface").mkdir(parents=True, exist_ok=True)
            (self.docling_cache_dir / "transformers").mkdir(parents=True, exist_ok=True)
            (self.docling_cache_dir / "torch").mkdir(parents=True, exist_ok=True)
            
            logger.info(f"Docling environment setup completed. Cache dir: {self.docling_cache_dir}")
            
        except Exception as e:
            logger.warning(f"Failed to setup Docling environment: {e}")
    
    def get_document_converter(self) -> DocumentConverter:
        """Get or initialize Docling DocumentConverter with Japanese support"""
        if self._document_converter is None:
            try:
                logger.info("Initializing Docling DocumentConverter with EasyOCR for better layout detection...")

                # Docling 2.65.0 API ã‚’ä½¿ç”¨
                from docling.document_converter import PdfFormatOption
                from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
                from docling.datamodel.base_models import InputFormat

                # EasyOCRã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå¢ƒç•Œãƒœãƒƒã‚¯ã‚¹æ¤œå‡ºï¼‰
                use_gpu = GPU_AVAILABLE
                ocr_options = EasyOcrOptions(
                    lang=['ja', 'en'],  # æ—¥æœ¬èªã¨è‹±èªã‚’æŒ‡å®šï¼ˆEasyOCRã®è¨€èªã‚³ãƒ¼ãƒ‰ï¼‰
                    use_gpu=use_gpu,  # GPU auto-detection
                    force_full_page_ocr=True,  # éæ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾ç­–ï¼šå…¨ãƒšãƒ¼ã‚¸ã§OCRã‚’å¼·åˆ¶å®Ÿè¡Œ
                )
                logger.info(f"Docling OCR configured with GPU: {use_gpu}, force_full_page_ocr=True")

                # PDFãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
                pdf_pipeline_options = PdfPipelineOptions(
                    do_ocr=True,  # OCRã‚’æœ‰åŠ¹åŒ–
                    ocr_options=ocr_options,
                    do_table_structure=True,  # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ è§£æã‚’æœ‰åŠ¹
                )

                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
                format_options = {
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options)
                }

                # DocumentConverterã‚’åˆæœŸåŒ–
                self._document_converter = DocumentConverter(format_options=format_options)

                logger.info("DocumentConverter initialized with EasyOCR for enhanced layout extraction")
            except ImportError as import_e:
                logger.warning(f"EasyOCR not available: {import_e}")
                # TesseractOCRã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                try:
                    logger.info("Falling back to Tesseract OCR...")

                    from docling.document_converter import PdfFormatOption
                    from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions
                    from docling.datamodel.base_models import InputFormat

                    # æ—¥æœ¬èªOCRã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆTesseractCLIä½¿ç”¨ï¼‰
                    ocr_options = TesseractCliOcrOptions(
                        lang=['jpn', 'eng'],  # æ—¥æœ¬èªã¨è‹±èªã‚’æŒ‡å®š
                        tesseract_cmd='tesseract',  # Tesseractã‚³ãƒãƒ³ãƒ‰æŒ‡å®š
                        force_full_page_ocr=True,  # éæ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾ç­–ï¼šå…¨ãƒšãƒ¼ã‚¸ã§OCRã‚’å¼·åˆ¶å®Ÿè¡Œ
                    )

                    # PDFãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
                    pdf_pipeline_options = PdfPipelineOptions(
                        do_ocr=True,  # OCRã‚’æœ‰åŠ¹åŒ–
                        ocr_options=ocr_options,
                        do_table_structure=True,  # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ è§£æã‚’æœ‰åŠ¹
                    )

                    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
                    format_options = {
                        InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options)
                    }

                    # DocumentConverterã‚’åˆæœŸåŒ–
                    self._document_converter = DocumentConverter(format_options=format_options)

                    logger.info("DocumentConverter initialized with Tesseract OCR")

                except Exception as e:
                    logger.warning(f"Failed to initialize DocumentConverter with OCR settings: {e}")
                    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå®Œå…¨ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    self._document_converter = DocumentConverter()
                    logger.info("DocumentConverter initialized with complete default settings")

        return self._document_converter

    def _log_docling_progress(self, description: str, step: int, total: int, start_time: float = None, progress_callback=None):
        """Doclingå°‚ç”¨ã®é€²æ—ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        current_time = time.time()
        elapsed = round(current_time - start_time, 2) if start_time else 0

        # progress_callbackã‚’å‘¼ã³å‡ºã—
        if progress_callback:
            progress_callback(step=step, total=total, description=description)

        # è©³ç´°ãªé€²æ—ãƒ­ã‚°ã‚’å‡ºåŠ›
        progress_log = {
            "timestamp": datetime.now().isoformat(),
            "component": "DoclingProcessor",
            "operation": "docling_conversion",
            "step": step,
            "total_steps": total,
            "percentage": round((step / total) * 100, 1) if total > 0 else 0,
            "description": description,
            "elapsed_seconds": elapsed,
            "frontend_format": {
                "status_text": description,
                "step_display": f"ã‚¹ãƒ†ãƒƒãƒ— {step}/{total}",
                "percentage_display": f"{(step / total) * 100:.1f}%" if total > 0 else "0.0%"
            }
        }
        logger.info(f"ğŸ”§ DOCLING_PROGRESS: {json.dumps(progress_log, ensure_ascii=False)}")

    def convert_document(self, document_path: str, progress_callback=None, output_dir: str = None) -> Any:
        """Convert document using Docling with PDF preprocessing fallback"""
        try:
            start_time = time.time()
            logger.info(f"Converting document with Docling: {document_path}")

            # Step 3: Doclingã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
            self._log_docling_progress(
                "Doclingã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–ä¸­...",
                step=3, total=10, start_time=start_time, progress_callback=progress_callback
            )

            converter = self.get_document_converter()

            # Step 4: å¤‰æ›é–‹å§‹
            self._log_docling_progress(
                f"{Path(document_path).name} ã®å¤‰æ›ã‚’é–‹å§‹...",
                step=4, total=10, start_time=start_time, progress_callback=progress_callback
            )

            logger.info(f"Starting Docling conversion for: {Path(document_path).name}")
            
            # PDFå‰å‡¦ç†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’ä½¿ç”¨
            from .pdf_preprocessor import PDFPreprocessor
            
            preprocessor = PDFPreprocessor(converter, output_dir)
            
            # ãƒ„ãƒ¼ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            tools_status = PDFPreprocessor.check_tools_availability()
            logger.info(f"PDF preprocessing tools status: {tools_status}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ
            self._log_docling_progress(
                "PDFå‰å‡¦ç†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œä¸­...",
                step=4, total=10, start_time=start_time, progress_callback=progress_callback
            )

            success, document, method_used = preprocessor.process_with_fallback(
                document_path, progress_callback
            )

            if success:
                self._log_docling_progress(
                    f"Doclingå¤‰æ›ãŒæˆåŠŸã—ã¾ã—ãŸ ({method_used})",
                    step=5, total=10, start_time=start_time, progress_callback=progress_callback
                )
                
                logger.info(f"Conversion successful using {method_used}: {type(document)}")
                logger.info(f"Document has {len(document.pages) if hasattr(document, 'pages') else 0} pages")
                
                # å…ƒã®PDFãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆç”»åƒç”Ÿæˆã§ä½¿ç”¨ï¼‰
                document._original_pdf_path = document_path
                document._processing_method = method_used
                
                logger.info(f"Docling conversion completed successfully with {method_used}")
                
                # assembled elements ã®æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                if hasattr(document, 'assembled') and document.assembled:
                    if hasattr(document.assembled, 'elements'):
                        logger.info(f"Assembled elements: {len(document.assembled.elements)}")
                        for i, element in enumerate(document.assembled.elements[:5]):  # æœ€åˆã®5å€‹ã‚’ç¢ºèª
                            logger.info(f"Element {i}: {type(element).__name__}")
                
                return document
            else:
                logger.error("All PDF preprocessing methods failed")
                raise RuntimeError("All PDF preprocessing and Docling conversion methods failed")
            
        except Exception as e:
            logger.error(f"Docling conversion failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def extract_document_metadata(self, document) -> Dict[str, Any]:
        """Extract metadata from Docling document"""
        try:
            # Docling 2.65.0+: DoclingDocument ã¾ãŸã¯ ExportedCCSDocument ã‚’åˆ¤å®š
            doc_type_name = type(document).__name__
            is_docling_doc = doc_type_name in ('DoclingDocument', 'ExportedCCSDocument')

            if is_docling_doc:
                # page_dimensionsã‹ã‚‰å®Ÿéš›ã®ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—
                actual_pages = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                if hasattr(document, 'page_dimensions') and document.page_dimensions:
                    actual_pages = len(document.page_dimensions)
                    logger.info(f"{doc_type_name} has {actual_pages} pages from page_dimensions")
                elif hasattr(document, 'pages') and document.pages:
                    actual_pages = len(document.pages)
                    logger.info(f"{doc_type_name} has {actual_pages} pages from pages attribute")

                metadata = {
                    "total_pages": actual_pages,
                    "processing_mode": "docling",
                    "docling_version": "2.65.0+",
                    "document_type": doc_type_name,
                    "elements_count": 0,
                    "element_types": {}
                }

                # ãƒ†ã‚­ã‚¹ãƒˆã‚„ãã®ä»–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if hasattr(document, 'main_text') and document.main_text:
                    metadata["has_main_text"] = True
                if hasattr(document, 'texts') and list(document.texts):
                    metadata["texts_count"] = len(list(document.texts))
                if hasattr(document, 'body') and document.body.children:
                    metadata["body_children_count"] = len(document.body.children)

                return metadata
            
            # å¾“æ¥ã®Documentç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = {
                "total_pages": len(document.pages) if hasattr(document, 'pages') else 0,
                "processing_mode": "docling",
                "docling_version": "1.7.0+",
                "document_type": "legacy",
                "elements_count": 0,
                "element_types": []
            }
            
            # assembled elements ã®çµ±è¨ˆæƒ…å ±
            if hasattr(document, 'assembled') and document.assembled:
                if hasattr(document.assembled, 'elements'):
                    metadata["elements_count"] = len(document.assembled.elements)
                    
                    # è¦ç´ ã‚¿ã‚¤ãƒ—ã®çµ±è¨ˆ
                    element_types = {}
                    for element in document.assembled.elements:
                        element_type = type(element).__name__
                        element_types[element_type] = element_types.get(element_type, 0) + 1
                    
                    metadata["element_types"] = element_types
            
            return metadata
            
        except Exception as e:
            logger.warning(f"Failed to extract document metadata: {e}")
            return {
                "total_pages": 0,
                "processing_mode": "docling",
                "error": str(e)
            }
    
    def extract_raw_pages_data(self, document) -> Dict[str, Any]:
        """Extract raw pages data from Docling document"""
        try:
            # Docling 2.65.0+: DoclingDocument ã¾ãŸã¯ ExportedCCSDocument ã‚’åˆ¤å®š
            doc_type_name = type(document).__name__
            is_docling_doc = doc_type_name in ('DoclingDocument', 'ExportedCCSDocument')

            if is_docling_doc:
                raw_pages_data = {
                    "document_type": doc_type_name,
                    "extraction_timestamp": datetime.now().isoformat(),
                    "total_pages": 0,
                    "pages": []
                }

                # page_dimensionsã¾ãŸã¯pagesã‹ã‚‰å®Ÿéš›ã®ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—
                actual_pages = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                if hasattr(document, 'page_dimensions') and document.page_dimensions:
                    actual_pages = len(document.page_dimensions)
                    logger.info(f"Extracting raw pages data from {actual_pages} pages")
                
                raw_pages_data["total_pages"] = actual_pages
                
                # å„ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
                for page_idx in range(actual_pages):
                    page_data = {
                        "page_number": page_idx + 1,
                        "width": None,
                        "height": None,
                        "elements": []
                    }
                    
                    # ãƒšãƒ¼ã‚¸å¯¸æ³•ã‚’å–å¾—
                    if hasattr(document, 'page_dimensions') and document.page_dimensions:
                        if page_idx < len(document.page_dimensions):
                            dimensions = document.page_dimensions[page_idx]
                            if hasattr(dimensions, 'width') and hasattr(dimensions, 'height'):
                                page_data["width"] = dimensions.width
                                page_data["height"] = dimensions.height
                    
                    raw_pages_data["pages"].append(page_data)
                
                # å…±é€šã®è¦ç´ æŠ½å‡ºå‡¦ç†
                element_id_counter = 0
                
                def extract_elements_from_collection(collection_name, collection_items, expected_type):
                    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰è¦ç´ ã‚’æŠ½å‡ºã™ã‚‹å…±é€šå‡¦ç†"""
                    nonlocal element_id_counter
                    
                    if not collection_items:
                        return
                        
                    logger.info(f"Extracting {len(collection_items)} {collection_name} elements")
                    
                    for item in collection_items:
                        # ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹è¦ç´ ã®ã¿å‡¦ç†ï¼ˆç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                        text_content = getattr(item, 'text', '').strip()
                        if not text_content and expected_type != 'table' and expected_type != 'figure':
                            continue  # ãƒ†ãƒ¼ãƒ–ãƒ«ã¨å›³ä»¥å¤–ã¯ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        
                        # provãƒªã‚¹ãƒˆã‹ã‚‰ãƒšãƒ¼ã‚¸ç•ªå·ã¨bboxæƒ…å ±ã‚’å–å¾—
                        if hasattr(item, 'prov') and item.prov:
                            # æœ€åˆã®provã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®ã¿ã‚’ä½¿ç”¨ï¼ˆé‡è¤‡ã‚’é˜²ãï¼‰
                            prov_item = item.prov[0]
                            page_num = getattr(prov_item, 'page', 1)
                            bbox_list = getattr(prov_item, 'bbox', [])
                            
                            if 1 <= page_num <= actual_pages:
                                page_idx = page_num - 1
                                
                                # obj_typeã«åŸºã¥ã„ã¦é©åˆ‡ãªtypeã‚’æ±ºå®š
                                obj_type = getattr(item, 'obj_type', None)
                                element_type = expected_type
                                
                                # obj_typeã«åŸºã¥ã„ã¦typeã‚’èª¿æ•´
                                if obj_type:
                                    if 'title' in obj_type.lower() or 'subtitle' in obj_type.lower():
                                        element_type = 'title'
                                    elif obj_type in ['header', 'page-header']:
                                        element_type = 'page_header'
                                    elif obj_type in ['footer', 'page-footer']:
                                        element_type = 'page_footer'
                                
                                element_data = {
                                    "element_id": element_id_counter,
                                    "type": element_type,
                                    "text": text_content if text_content else f"[{element_type.title()}]",
                                    "bbox": {}
                                }
                                
                                # bounding boxæƒ…å ±ã‚’æŠ½å‡ºï¼ˆ[x1, y1, x2, y2]å½¢å¼ï¼‰
                                if bbox_list and len(bbox_list) >= 4:
                                    element_data["bbox"] = {
                                        "x1": bbox_list[0],
                                        "y1": bbox_list[1], 
                                        "x2": bbox_list[2],
                                        "y2": bbox_list[3]
                                    }
                                
                                # è¿½åŠ å±æ€§ãŒã‚ã‚Œã°å«ã‚ã‚‹
                                if obj_type:
                                    element_data["obj_type"] = obj_type
                                
                                raw_pages_data["pages"][page_idx]["elements"].append(element_data)
                                element_id_counter += 1
                                
                                # ãƒ†ãƒ¼ãƒ–ãƒ«è¦ç´ ã®å ´åˆã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ãƒ«ã‚’è¿½åŠ æŠ½å‡º
                                if expected_type == 'table' and hasattr(item, 'data'):
                                    extract_table_cells(item, page_idx, page_num)
                
                def extract_table_cells(table_element, page_idx: int, page_num: int):
                    """ãƒ†ãƒ¼ãƒ–ãƒ«è¦ç´ ã‹ã‚‰ã‚»ãƒ«ã‚’æŠ½å‡º"""
                    nonlocal element_id_counter
                    
                    try:
                        logger.info(f"Attempting to extract table cells from {type(table_element).__name__}")
                        logger.info(f"Table element attributes: {[attr for attr in dir(table_element) if not attr.startswith('_')]}")
                        
                        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’èª¿ã¹ã‚‹
                        if hasattr(table_element, 'data'):
                            logger.info(f"Table element has 'data' attribute: {type(table_element.data)}")
                            
                            # table.dataãŒç›´æ¥ãƒªã‚¹ãƒˆã®å ´åˆï¼ˆlayout_extractorã¨åŒã˜æ§‹é€ ï¼‰
                            if isinstance(table_element.data, list):
                                logger.info(f"Table has {len(table_element.data)} rows (direct list)")
                                for row_idx, row in enumerate(table_element.data):
                                    if isinstance(row, list):
                                        logger.info(f"Row {row_idx} has {len(row)} cells")
                                        for col_idx, cell in enumerate(row):
                                            # ã‚»ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                                            cell_text = getattr(cell, 'text', '').strip()
                                            logger.info(f"Cell [{row_idx}][{col_idx}]: '{cell_text}'")
                                            
                                            if cell_text:  # ç©ºã§ãªã„ã‚»ãƒ«ã®ã¿
                                                    
                                                    # ã‚»ãƒ«ã®bboxæƒ…å ±ã‚’å–å¾—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
                                                    cell_bbox = {}
                                                    # ã‚»ãƒ«ã®bboxæƒ…å ±ã‚’å–å¾—ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰
                                                    if hasattr(cell, 'bbox') and cell.bbox:
                                                        bbox = cell.bbox
                                                        if isinstance(bbox, list) and len(bbox) >= 4:
                                                            cell_bbox = {
                                                                "x1": float(bbox[0]),
                                                                "y1": float(bbox[1]), 
                                                                "x2": float(bbox[2]),
                                                                "y2": float(bbox[3])
                                                            }
                                                        elif hasattr(bbox, 'l') and hasattr(bbox, 't') and hasattr(bbox, 'r') and hasattr(bbox, 'b'):
                                                            cell_bbox = {
                                                                "x1": float(bbox.l),
                                                                "y1": float(bbox.t), 
                                                                "x2": float(bbox.r),
                                                                "y2": float(bbox.b)
                                                            }
                                                    
                                                    cell_data = {
                                                        "element_id": element_id_counter,
                                                        "type": "table_cell",
                                                        "text": cell_text,
                                                        "bbox": cell_bbox,
                                                        "table_info": {
                                                            "row": row_idx,
                                                            "col": col_idx,
                                                            "cell_type": getattr(cell, 'obj_type', 'body'),
                                                            "is_header": getattr(cell, 'col_header', False) or getattr(cell, 'row_header', False)
                                                        }
                                                    }
                                                    
                                                    raw_pages_data["pages"][page_idx]["elements"].append(cell_data)
                                                    element_id_counter += 1
                                                    logger.info(f"Added table cell: {cell_text}")
                                    else:
                                        logger.info(f"Row {row_idx} is not a list")
                            
                            # table.data.tableã®æ§‹é€ ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ¥ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆï¼‰
                            elif hasattr(table_element.data, 'table'):
                                logger.info(f"Table data has 'table' attribute: {type(table_element.data.table)}")
                                # å¾“æ¥ã®å‡¦ç†ã‚’ã“ã“ã«ä¿æŒï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                        else:
                            logger.info("Table element has no 'data' attribute")
                            
                    except Exception as e:
                        logger.warning(f"Failed to extract table cells: {e}")
                        import traceback
                        logger.warning(f"Traceback: {traceback.format_exc()}")
                
                # åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                available_collections = []
                for attr_name in dir(document):
                    if not attr_name.startswith('_'):
                        attr_value = getattr(document, attr_name, None)
                        if hasattr(attr_value, '__len__') and not callable(attr_value):
                            try:
                                if len(attr_value) > 0:
                                    available_collections.append(f"{attr_name}({len(attr_value)})")
                            except:
                                pass
                logger.info(f"Available collections: {available_collections}")
                
                # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰è¦ç´ ã‚’æŠ½å‡º
                if hasattr(document, 'main_text'):
                    extract_elements_from_collection('main_text', document.main_text, 'text')
                
                # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰è¦ç´ ã‚’æŠ½å‡ºï¼ˆå…±é€šå‡¦ç†ã‚’ä½¿ç”¨ï¼‰
                if hasattr(document, 'tables'):
                    extract_elements_from_collection('tables', document.tables, 'table')
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ãƒ«ã‚‚æŠ½å‡ºï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
                if hasattr(document, 'table_cells'):
                    extract_elements_from_collection('table_cells', document.table_cells, 'table_cell')
                
                if hasattr(document, 'figures'):
                    extract_elements_from_collection('figures', document.figures, 'figure')
                
                if hasattr(document, 'page_headers'):
                    extract_elements_from_collection('page_headers', document.page_headers, 'page_header')
                
                if hasattr(document, 'page_footers'):
                    extract_elements_from_collection('page_footers', document.page_footers, 'page_footer')
                
                # titles ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚‚æŠ½å‡ºï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
                if hasattr(document, 'titles'):
                    extract_elements_from_collection('titles', document.titles, 'title')
                
                # å„ãƒšãƒ¼ã‚¸ã®è¦ç´ ã‚’Yåº§æ¨™ã§ä¸¦ã³æ›¿ãˆï¼ˆä¸Šã‹ã‚‰ä¸‹ã¸ï¼‰
                for page_data in raw_pages_data["pages"]:
                    page_data["elements"].sort(key=lambda x: x["bbox"].get("y1", 0) if x["bbox"] else 0, reverse=True)
                
                # å„ãƒšãƒ¼ã‚¸ã®è¦ç´ æ•°ã‚’é›†è¨ˆ
                total_elements = sum(len(page["elements"]) for page in raw_pages_data["pages"])
                logger.info(f"Raw pages data extracted: {actual_pages} pages with {total_elements} total elements")
                
                # å„ãƒšãƒ¼ã‚¸ã®è¦ç´ ã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
                for i, page_data in enumerate(raw_pages_data["pages"], 1):
                    if page_data["elements"]:
                        type_counts = {}
                        for element in page_data["elements"]:
                            element_type = element["type"]
                            type_counts[element_type] = type_counts.get(element_type, 0) + 1
                        logger.info(f"Page {i}: {len(page_data['elements'])} elements - {type_counts}")
                
                return raw_pages_data
            
            else:
                logger.warning(f"Unsupported document type for raw pages extraction: {type(document)}")
                return {
                    "document_type": str(type(document)),
                    "extraction_timestamp": datetime.now().isoformat(),
                    "total_pages": 0,
                    "pages": [],
                    "error": "Unsupported document type"
                }
                
        except Exception as e:
            logger.error(f"Failed to extract raw pages data: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "extraction_timestamp": datetime.now().isoformat(),
                "total_pages": 0,
                "pages": [],
                "error": str(e)
            }
    
    def cleanup(self):
        """Cleanup Docling resources"""
        try:
            if self._document_converter:
                # DocumentConverterã«ã¯æ˜ç¤ºçš„ãªcleanupãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„ãŸã‚ã€
                # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤ã—ã¦ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ä»»ã›ã‚‹
                self._document_converter = None
                gc.collect()
                logger.info("Docling DocumentConverter cleanup completed")
        except Exception as e:
            logger.warning(f"Failed to cleanup Docling resources: {e}")
    
    def validate_document(self, document) -> bool:
        """Validate Docling document structure (ExportedCCSDocument)"""
        try:
            # åŸºæœ¬çš„ãªæ¤œè¨¼
            if not document:
                logger.warning("Document is None")
                return False
            
            # Docling 2.65.0+: DoclingDocument ã¾ãŸã¯ ExportedCCSDocument ã®æ¤œè¨¼
            try:
                from docling_core.types.doc.document import DoclingDocument
                if isinstance(document, DoclingDocument):
                    # DoclingDocumentã¯å¸¸ã«æœ‰åŠ¹ã¨ã¿ãªã™
                    logger.info(f"DoclingDocument validation passed: {type(document)}")
                    return True
            except ImportError:
                pass

            try:
                from docling_core.types.doc.document import ExportedCCSDocument
                if isinstance(document, ExportedCCSDocument):
                    # ExportedCCSDocumentã¯å¸¸ã«æœ‰åŠ¹ã¨ã¿ãªã™ï¼ˆæ§‹é€ ãŒç•°ãªã‚‹ãŸã‚ï¼‰
                    logger.info(f"ExportedCCSDocument validation passed: {type(document)}")
                    return True
            except ImportError:
                pass

            # å‹åã§ã®åˆ¤å®šï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            doc_type_name = type(document).__name__
            if doc_type_name in ('DoclingDocument', 'ExportedCCSDocument'):
                logger.info(f"{doc_type_name} validation passed (by name)")
                return True
            
            # å¾“æ¥ã®Documentç”¨ã®æ¤œè¨¼ï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
            if not hasattr(document, 'pages'):
                logger.warning("Document has no pages attribute")
                return False
            
            if len(document.pages) == 0:
                logger.warning("Document has no pages")
                return False
            
            # assembled elements ã®æ¤œè¨¼
            if hasattr(document, 'assembled') and document.assembled:
                if hasattr(document.assembled, 'elements'):
                    if len(document.assembled.elements) == 0:
                        logger.warning("Document has no assembled elements")
                    else:
                        logger.info(f"Document validation passed: {len(document.pages)} pages, {len(document.assembled.elements)} elements")
                        return True
            
            logger.warning("Document has no assembled elements")
            return True  # ãƒšãƒ¼ã‚¸ãŒã‚ã‚Œã°æœ‰åŠ¹ã¨ã¿ãªã™
            
        except Exception as e:
            logger.error(f"Document validation failed: {e}")
            return False